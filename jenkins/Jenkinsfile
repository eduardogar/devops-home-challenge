// This Jenkinsfile defines a declarative pipeline for building, testing, and deploying a Docker application.
// The pipeline is configured to run on the Jenkins master node using 'agent any'.
pipeline {
    // The pipeline will run on any available agent (in this case, the Jenkins master container).
    agent any

    // Define environment variables that are available to all stages.
    environment {
        REPO_URL           = 'https://github.com/eduardogar/devops-home-challenge.git'
        BRANCH_NAME        = 'main'
        REGISTRY_URL       = 'host.docker.internal:5000'
        APP_NAME           = 'devops-challenge-app'
        DOCKER_IMAGE_VERSIONED = "${REGISTRY_URL}/${APP_NAME}:${env.BUILD_NUMBER}"
        DOCKER_IMAGE_LATEST    = "${REGISTRY_URL}/${APP_NAME}:latest"
        DOCKER_IMAGE_STABLE    = "${REGISTRY_URL}/${APP_NAME}:stable"
    }

    stages {
        // Stage 1: Checkout the source code from the repository.
        stage('Code Checkout') {
            steps {
                // This step clears the workspace before a new build starts.
                cleanWs()

                // This step performs the equivalent of 'git clone' using the Jenkins Git plugin,
                // using the specified branch from the environment variables.
                git url: "${REPO_URL}", branch: "${BRANCH_NAME}"
            }
        }

        // Stage 2: Perform a security scan on the Infrastructure as Code (IaC) files using tfsec.
        stage('IaC Security Scan') {
            steps {
                sh 'tfsec ./terraform'
            }
        }

        // NEW: Stage 2.5 Ensure Docker network and API relay exist (idempotent)
        stage('Docker Network & API Relay') {
            steps {
                sh """
                set -e

                # Create network (no-op if it already exists)
                docker network create devops-net || true

                # Attach containers (no-op if already attached)
                docker network connect devops-net jenkins || true
                docker network connect devops-net local-registry || true
                docker network connect devops-net devops-challenge || true

                # Recreate API relay that exposes 0.0.0.0:8443 -> 127.0.0.1:8443 in the minikube node
                docker rm -f k8s-apiserver-relay 2>/dev/null || true
                docker run -d --name k8s-apiserver-relay --restart unless-stopped \\
                  --network container:devops-challenge alpine/socat \\
                  TCP-LISTEN:8443,fork,reuseaddr TCP:127.0.0.1:8443

                # Quick reachability probe (prints NOT reachable if fails)
                bash -lc 'getent hosts devops-challenge; (echo > /dev/tcp/devops-challenge/8443) >/dev/null 2>&1 && echo "API :8443 reachable" || echo "API :8443 NOT reachable"'
                """
            }
        }

        // Stage 3: Configure the Kubernetes Context.
        stage('Configure Kubernetes Context') {
            steps{
                sh """
                # Set KUBECONFIG to a new file in the workspace
                export KUBECONFIG=${WORKSPACE}/kubeconfig
                
                # Now, generate a corrected kubeconfig file that uses the Minikube IP address and
                # embeds the client certificates and keys directly.
                cat > ${WORKSPACE}/kubeconfig << EOF
apiVersion: v1
clusters:
- cluster:
    server: https://devops-challenge:8443
    insecure-skip-tls-verify: true
  name: devops-challenge
contexts:
- context:
    cluster: devops-challenge
    namespace: default
    user: devops-challenge
  name: devops-challenge
current-context: devops-challenge
kind: Config
preferences: {}
users:
- name: devops-challenge
  user:
    client-certificate-data: \$(cat /var/jenkins_home/.minikube/profiles/devops-challenge/client.crt | base64 -w0)
    client-key-data: \$(cat /var/jenkins_home/.minikube/profiles/devops-challenge/client.key | base64 -w0)
EOF
                # Now that the context is created, we can use it.
                kubectl config use-context devops-challenge
                kubectl --kubeconfig "$WORKSPACE/kubeconfig" cluster-info
                kubectl --kubeconfig "$WORKSPACE/kubeconfig" get nodes

                kubectl get nodes
                
                # make sure namespaces exist (idempotent)
                kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -
                kubectl create ns production --dry-run=client -o yaml | kubectl apply -f -
                """
            }
        }

        // Stage 4: Build and tag the Docker image.
        stage('Docker Build & Tag') {
            steps {
                script {
                    echo "Building and tagging new image..."
                    // Build the Docker image from the application's Dockerfile.
                    sh "docker build --build-arg APP_VERSION=v1.${env.BUILD_NUMBER} -t ${DOCKER_IMAGE_VERSIONED} ./application"
                    
                    echo "Updating 'stable' tag..."
                    // This block attempts to update the 'stable' tag, which is used for production deployments.
                    // It's expected to fail gracefully on the first run.
                    try {
                        sh "docker pull ${DOCKER_IMAGE_LATEST}"
                        sh "docker tag ${DOCKER_IMAGE_LATEST} ${DOCKER_IMAGE_STABLE}"
                        sh "docker push ${DOCKER_IMAGE_STABLE}"
                    } catch (e) {
                        echo "Could not re-tag previous 'latest' to 'stable'. This is expected on the first run."
                    }

                    echo "Updating 'latest' tag..."
                    // Tag the new versioned image as 'latest'.
                    sh "docker tag ${DOCKER_IMAGE_VERSIONED} ${DOCKER_IMAGE_LATEST}"
                    
                    echo "Pushing all new tags to the local Docker registry..."
                    // Push the newly created tags to the local Docker registry.
                    sh "docker push ${DOCKER_IMAGE_VERSIONED}"
                    sh "docker push ${DOCKER_IMAGE_LATEST}"
                }
            }
        }

        // Stage 5: Scan the newly built Docker image for vulnerabilities using Trivy.
        stage('Image Security Scan') {
            steps {
                script {
                    echo "Scanning versioned image for vulnerabilities..."
                    // Run a Trivy scan on the versioned image, exiting with code 0 even if vulnerabilities are found
                    // but logging them to the console.
                    sh "trivy image --exit-code 0 --severity HIGH,CRITICAL ${DOCKER_IMAGE_VERSIONED}"
                }
            }
        }

        // Stage 6: Syntax check.
        stage('Helm Lint/Template') {
            steps{
                sh """
                ls -l kubernetes/helm-chart/
                ls -l kubernetes/helm-chart/values.yaml
                cat kubernetes/helm-chart/values.yaml
                cat kubernetes/helm-chart/templates/service.yaml
                helm lint ./kubernetes/helm-chart
                helm template my-app-staging ./kubernetes/helm-chart \\
                -f kubernetes/helm-chart/values.yaml \\
                --set image.tag='test' --set replicaCount=1 \\
                --set config.welcomeMessage="Template Smoke Test"
                """
            }
        }

        // Stage 7: Deploy to Staging.
        stage('Deploy to Staging') {
            steps {
                echo 'Deploying application to the staging namespace using Helm...'
                sh """
                    helm upgrade --install my-app-staging ./kubernetes/helm-chart \
                      --namespace staging \
                      --set image.repository=${REGISTRY_URL}/${APP_NAME} \
                      --set image.tag=${env.BUILD_NUMBER} \
                      --set replicaCount=1 \
                      --set-string config.welcomeMessage="Welcome to the STAGING Environment!" \
                      --set service.port=80 \
                      --set service.targetPort=8080 \
                      --set ingress.enabled=true \
                      --set ingress.className=nginx \
                      --set ingress.host=my-app-staging.local \
                      --set ingress.path=/ \
                      --set ingress.pathType=Prefix \
                      --kubeconfig ${WORKSPACE}/kubeconfig
                """
            }
        }

        // Stage 8: Manual approval for promotion to production.
        stage('Approval for Production') {
            steps {
                input message: 'Deploy to Production?', ok: 'Yes'
            }
        }

        // Stage 9: Deploy the stable version of the application to the production environment.
        stage('Deploy to Production') {
            steps {
                echo 'Deploying application to the production namespace using Helm...'
                sh """
                    helm upgrade --install my-app-production ./kubernetes/helm-chart \
                      --namespace production \
                      --set image.repository=${REGISTRY_URL}/${APP_NAME} \
                      --set image.tag=stable \
                      --set replicaCount=3 \
                      --set-string config.welcomeMessage="Welcome to the PRODUCTION Environment!" \
                      --set service.port=80 \
                      --set service.targetPort=8080 \
                      --set ingress.enabled=true \
                      --set ingress.className=nginx \
                      --set ingress.host=my-app.local \
                      --set ingress.path=/ \
                      --set ingress.pathType=Prefix \
                      --kubeconfig ${WORKSPACE}/kubeconfig
                """
            }
        }
    }
    // This 'post' section runs after the build finishes, regardless of the outcome.
    post {
        // The 'always' condition ensures the 'cleanWs' step is executed every time.
        always {
            echo "Cleaning up the workspace..."
            // The 'cleanWs' step removes all files from the workspace,
            // preparing it for the next build.
            cleanWs()
        }
    }
}
